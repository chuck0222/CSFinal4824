{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259f5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe205d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from CSV files\n",
    "train = pd.read_csv(\"Dataset/train.csv\")\n",
    "test = pd.read_csv(\"Dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f016cee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop subject/date and concatenating text and title\n",
    "train[\"text\"] = train[\"title\"] + \" \" + train[\"text\"]\n",
    "test[\"text\"] = test[\"title\"] + \" \" + test[\"text\"]\n",
    "\n",
    "train = train.drop([\"subject\", \"date\", \"title\"], axis = 1)\n",
    "test = test.drop([\"subject\", \"date\", \"title\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ceff00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def clean_text_data(data_point):\n",
    "    review_soup = BeautifulSoup(data_point)\n",
    "    review_text = review_soup.get_text()\n",
    "    review_letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    review_lower_case = review_letters_only.lower()  \n",
    "    review_words = review_lower_case.split() \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    meaningful_words = [x for x in review_words if x not in stop_words]\n",
    "        \n",
    "    return(\" \".join(meaningful_words)) \n",
    "\n",
    "train[\"text\"] = train[\"text\"].apply(clean_text_data)\n",
    "test[\"text\"] = test[\"text\"].apply(clean_text_data)\n",
    "data = pd.concat([train, test], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c57b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train[\"text\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67046edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train[\"text\"], train[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert train/test data to vectors\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) \n",
    "\n",
    "# Convert the text into bag-of-words features\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data[\"text\"])\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "\n",
    "# Convert the sparse matrices to dense numpy arrays and add an extra dimension\n",
    "X_train_array = np.expand_dims(X_train.toarray(), axis=-1)\n",
    "X_val_array = np.expand_dims(X_val.toarray(), axis=-1)\n",
    "\n",
    "# Convert the labels to numpy arrays\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_val = Y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637a527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_val = le.transform(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57f700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "499/499 [==============================] - 3530s 7s/step - loss: 0.0632 - accuracy: 0.9836 - precision: 0.9803 - recall: 0.9852 - val_loss: 0.0417 - val_accuracy: 0.9868 - val_precision: 0.9973 - val_recall: 0.9747\n",
      "Epoch 2/3\n",
      "499/499 [==============================] - 3421s 7s/step - loss: 0.0101 - accuracy: 0.9975 - precision: 0.9972 - recall: 0.9975 - val_loss: 0.0127 - val_accuracy: 0.9970 - val_precision: 0.9957 - val_recall: 0.9979\n",
      "Epoch 3/3\n",
      "499/499 [==============================] - 3324s 7s/step - loss: 0.0045 - accuracy: 0.9990 - precision: 0.9989 - recall: 0.9989 - val_loss: 0.0133 - val_accuracy: 0.9970 - val_precision: 0.9960 - val_recall: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2778011e460>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input shape\n",
    "input_shape = X_train_array.shape[1:]\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(np.unique(Y_train))\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), \n",
    "                       tf.keras.metrics.Recall(name='recall')])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_array, Y_train, validation_data=(X_val_array, Y_val), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04564636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 128s 813ms/step - loss: 0.0052 - accuracy: 0.9976 - precision: 0.9996 - recall: 0.9956\n",
      "Test accuracy: 0.9976000189781189\n"
     ]
    }
   ],
   "source": [
    "X_test = test[\"text\"]\n",
    "X_test = vectorizer.transform(X_test)\n",
    "y_test = test['label']\n",
    "y_test = le.transform(y_test)\n",
    "# Reshape the array\n",
    "X_test = np.expand_dims(X_test.toarray(), axis=-1)\n",
    "# Evaluate the CNN model on the testing data\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d89d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9975951780962218\n"
     ]
    }
   ],
   "source": [
    "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "print(f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
