{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import csr_matrix\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from CSV files\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop subject/date and concatenating text and title\n",
    "train[\"text\"] = train[\"title\"] + \" \" + train[\"text\"]\n",
    "test[\"text\"] = test[\"title\"] + \" \" + test[\"text\"]\n",
    "\n",
    "train = train.drop([\"subject\", \"date\", \"title\"], axis = 1)\n",
    "test = test.drop([\"subject\", \"date\", \"title\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define a function to clean the text data\n",
    "def clean_text_data(data_point):\n",
    "    # Use BeautifulSoup to parse the input text\n",
    "    review_soup = BeautifulSoup(data_point)\n",
    "\n",
    "    #Extract the text and do cleaning of data\n",
    "    review_text = review_soup.get_text()\n",
    "    review_letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    review_lower_case = review_letters_only.lower()  \n",
    "    review_words = review_lower_case.split() \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    meaningful_words = [x for x in review_words if x not in stop_words]\n",
    "\n",
    "    # Join the meaningful words back into a single string and return the result\n",
    "    return(\" \".join(meaningful_words)) \n",
    "\n",
    "train[\"text\"] = train[\"text\"].apply(clean_text_data)\n",
    "test[\"text\"] = test[\"text\"].apply(clean_text_data)\n",
    "data = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train[\"text\"], train[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert train/test data to vectors\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) \n",
    "\n",
    "# Convert the text into bag-of-words features\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data[\"text\"])\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "\n",
    "# Convert the sparse matrices to dense numpy arrays and add an extra dimension\n",
    "X_train_array = np.expand_dims(X_train.toarray(), axis=-1)\n",
    "X_val_array = np.expand_dims(X_val.toarray(), axis=-1)\n",
    "\n",
    "# Convert the labels to numpy arrays\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_val = Y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_val = le.transform(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTINOMIAL NAIVE BAYES CLASS USED FOR ML MODEL\n",
    "class MultinomialNaiveBayes:\n",
    "    # Initialize the Multinomial Naive Bayes classifier with a smoothing parameter alpha\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Determine the unique classes and their count\n",
    "        self.classes = np.unique(y)\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        # Initialize arrays for feature counts and class counts\n",
    "        self.feature_counts = np.zeros((self.num_classes, X.shape[1]))\n",
    "        self.class_counts = np.zeros(self.num_classes)\n",
    "\n",
    "        # Calculate feature counts and class counts for each class\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.feature_counts[i, :] = np.sum(X_c, axis=0) + self.alpha\n",
    "            self.class_counts[i] = X_c.shape[0]\n",
    "\n",
    "        # Compute the log probabilities for features and classes\n",
    "        self.feature_log_prob = np.log(self.feature_counts / self.feature_counts.sum(axis=1, keepdims=True))\n",
    "        self.class_log_prior = np.log(self.class_counts / self.class_counts.sum())\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Calculate log likelihood for each class, using feature log probabilities and class log prior\n",
    "        log_likelihood = X @ self.feature_log_prob.T + self.class_log_prior\n",
    "        # Return the class with the highest log likelihood for each sample\n",
    "        return self.classes[np.argmax(log_likelihood, axis=1)]\n",
    "\n",
    "        # # Calculate log likelihood for each class, using feature log probabilities and class log prior\n",
    "        # log_likelihood = X @ self.feature_log_prob.T + self.class_log_prior\n",
    "        # # Return the class with the highest log likelihood for each sample\n",
    "        # return np.argmax(log_likelihood, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train[\"text\"], train[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object with sparse=True\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english', max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the vectorizer on the training data\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data using the fitted vectorizer\n",
    "X_val_counts = vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9605263157894737\n",
      "Test accuracy: 0.9646\n"
     ]
    }
   ],
   "source": [
    "# Train the Naive Bayes model\n",
    "naive_bayes = MultinomialNaiveBayes()\n",
    "naive_bayes.fit(X_train_counts, y_train)\n",
    "\n",
    "# Validate the Naive Bayes model\n",
    "y_val_pred = naive_bayes.predict(X_val_counts)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation accuracy:\", val_accuracy)\n",
    "\n",
    "# Evaluate the Naive Bayes model on the test data\n",
    "X_test_counts = vectorizer.transform(test[\"text\"])\n",
    "y_test = test[\"label\"]\n",
    "\n",
    "y_test_pred = naive_bayes.predict(X_test_counts)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation recall: 0.9618869936034116\n",
      "Test recall: 0.9368\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall score for the validation data\n",
    "val_recall = recall_score(y_val, y_val_pred, average='binary', pos_label='real')\n",
    "print(\"Validation recall:\", val_recall)\n",
    "\n",
    "# Calculate recall score for the test data\n",
    "test_recall = recall_score(y_test, y_test_pred, average='binary', pos_label='real')\n",
    "print(\"Test recall:\", test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.9581839904420549\n",
      "Test F1 score: 0.9635877391483234\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score for the validation data\n",
    "val_f1 = f1_score(y_val, y_val_pred, average='binary', pos_label='real')\n",
    "print(\"Validation F1 score:\", val_f1)\n",
    "\n",
    "# Calculate F1 score for the test data\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='binary', pos_label='real')\n",
    "print(\"Test F1 score:\", test_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
